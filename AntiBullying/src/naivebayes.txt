\section{Naive Bayes}
Naive Bayes is a popular in its simplicity probabilistic classifier which is used across many fields within computer science. In its essence the model learns the conditional probabilities between the class and a classified instance, with assumed independence between the blocks of such instance. The label is assigned based on the greedy rule, such that the class which yields the highest probability is chosen as an assigned class. More formally:
$$ \hat{y} = argmax_{j \in J}(P(y_j|X)) = argmax_{j\in J}( P(y_j)P(x_0 | y_j) * P(y_j)P(x_1 | y_j) ... P(y_j)P(x_i | y_j) ) $$
where $\hat{y}$ is a predicted label, $argmax$ is a function which returns the argument which yields to the maximum value, $X$ is an input vector which consists of entries ${x_0 ... x_i}$ and $J$ is a set of all classes. As it is seen in the formula, the prediction of a true label is based only on the prior distribution and the conditional probability of every single element in the vector to the given label. Such simplicity attracts the attention of developers from many fields, however the assumption of independence hinders the algorithm. Although it achieves good performance in some "bag of words" tasks, it is known to under-perform in sequential data analysis since sequential data assumes dependence. In this project Gaussian version of Naive Bayes is used, with pre-determined probabilities of priors (due to the unbalanced data set). Different architectures were tested, resulting in higher accuracy, but since the main metric to be optimised is Recall, Gaussian Naive Bayes was chosen as the final model.